{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('uiux_reason.txt') as file:\n",
    "    original = file.read().split('.')\n",
    "corpus = original\n",
    "corpus[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "# Remove Punctuation\n",
    "\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r\"[.?!,;:-]\", \" \", text)\n",
    "    text = re.sub(r'[^\\w\\s]', \"\", text)\n",
    "    text = re.sub(' +', \" \", text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "# Remove Stopwords\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    filtered_words = [word.lower()\n",
    "                      for word in text.split() if word.lower() not in stop]\n",
    "    if filtered_words != []:\n",
    "        return \" \".join(filtered_words)\n",
    "    else:\n",
    "        return(text)\n",
    "\n",
    "\n",
    "# Get word Tag\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Word lemmatization\n",
    "\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    tagged_sent = pos_tag(tokens)\n",
    "\n",
    "    wnl = WordNetLemmatizer()\n",
    "    lemmas_sent = []\n",
    "    for tag in tagged_sent:\n",
    "        wordnet_pos = get_wordnet_pos(tag[1]) or wordnet.NOUN\n",
    "        lemmas_sent.append(wnl.lemmatize(tag[0], pos=wordnet_pos))  # 詞形還原\n",
    "\n",
    "    return ' '.join(lemmas_sent)\n",
    "\n",
    "\n",
    "def sentence_processing(sentence):\n",
    "    sentence = remove_punctuation(sentence)\n",
    "    sentence = remove_stopwords(sentence)\n",
    "    sentence = lemmatize_sentence(sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [sentence_processing(sentence) for sentence in corpus]\n",
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmath import nan\n",
    "from gensim.models import Word2Vec,KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz',binary=True,limit=100000)\n",
    "vec_corpus = [i.split() for i in corpus]\n",
    "\n",
    "def sentence2vec(model,sentence,selfTrain=False):\n",
    "    tmp = []\n",
    "    for word in sentence:\n",
    "        try:\n",
    "            tmp.append(model[word])\n",
    "        except:continue\n",
    "    tmp = np.array(tmp)\n",
    "    tmp = np.average(tmp,axis=0)\n",
    "    return tmp\n",
    "\n",
    "vec_vec=[]\n",
    "for s in vec_corpus:\n",
    "    vec_vec.append(sentence2vec(model,s))\n",
    "    \n",
    "Word2vec_google_matrix = np.array(vec_vec)[:-1]\n",
    "Word2vec_google_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Representative Score of sentence by Counting average Cosine Similarity with all vectors\n",
    "def scoring(main, all):  \n",
    "    sum = 0\n",
    "    main = np.array(main).reshape(1, -1)\n",
    "    all = np.array(all)\n",
    "    \n",
    "    for i in all:\n",
    "        sum += cosine_similarity(main, i.reshape(1, -1))\n",
    "        \n",
    "    avg = sum / len(all)\n",
    "    return avg\n",
    "\n",
    "\n",
    "# List with Representative Score from highest to lowest\n",
    "def get_matrix(corpus, matrix):\n",
    "\n",
    "    score_frame = []\n",
    "\n",
    "    for main in matrix:\n",
    "        main_score = scoring(np.array(main).reshape(1, -1), matrix)\n",
    "        score_frame.append(main_score[0][0])\n",
    "\n",
    "    matrix = list(zip(score_frame, matrix, corpus))\n",
    "    matrix.sort(key=lambda y: y[0], reverse=True)\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_refer = get_matrix(corpus, Word2vec_google_matrix)\n",
    "\n",
    "summa_list = [score_refer[0][2]]\n",
    "versus = [np.array(score_refer[0][1])]\n",
    "topics = 0\n",
    "for i in range(len(Word2vec_google_matrix)):\n",
    "    if topics <= 3:\n",
    "        present_chosen = np.array(score_refer[i][1]).reshape(1, -1)\n",
    "        qual_score = 0\n",
    "        for former_chosen in versus:\n",
    "            former_chosen = np.array(former_chosen).reshape(1, -1)\n",
    "            qual_score += cosine_similarity(present_chosen, former_chosen)\n",
    "\n",
    "        qual_score = qual_score / len(versus)\n",
    "\n",
    "        # if present sentences too similar with former sentence than reject it.\n",
    "        if qual_score[0][0] <= 0.5:\n",
    "            text = score_refer[i][2]\n",
    "            original_text = original[int(corpus.index(f'{text}'))]\n",
    "\n",
    "            print(\n",
    "                f'Text :\\033[0;33;40m {text}\\033[0m'.format(text),\n",
    "                f'Original Text: {original_text}',\n",
    "                f'Similarity of All : \\033[0;34;40m{round(score_refer[i][0],4)}\\033[0m',\n",
    "                f'Qulified Score : {qual_score[0][0]}',\n",
    "                '',\n",
    "                sep='\\n')\n",
    "\n",
    "            summa_list.append(score_refer[i][2])\n",
    "            versus.append(score_refer[i][1])\n",
    "            topics += 1\n",
    "\n",
    "        else:\n",
    "            continue\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
